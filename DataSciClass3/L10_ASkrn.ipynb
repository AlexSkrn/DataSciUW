{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10 Assignment - Keras LTSM\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The Keras Reuters newswire topics classification dataset. This dataset contains 11,228 newswires from Reuters, labeled with over 46 topics.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Using the <a href='https://keras.io/datasets/#reuters-newswire-topics-classification'>Keras</a> dataset, perform each of the following data preparation tasks and answer the related questions:\n",
    "\n",
    "1. Read Reuters dataset into training and testing.\n",
    "2. Prepare dataset.\n",
    "3. Build and compile 3 different models using Keras LTSM ideally improving model at each iteration.\n",
    "4. Describe and explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### IMPORTANT if numpy version is 1.16.3\n",
    ">\n",
    "> Edit the reuters.py file to solve ```ValueError: Object arrays cannot be loaded when allow_pickle=False``` which arises when numpy version is '1.16.3'\n",
    ">\n",
    "> On my Mac the path is:\n",
    "```Macintosh HD⁩ ▸ ⁨anaconda3⁩ ▸ ⁨lib⁩ ▸ ⁨python3.6⁩ ▸ ⁨site-packages⁩ ▸ ⁨tensorflow⁩ ▸ ⁨python⁩ ▸ ⁨keras⁩ ▸ ⁨datasets⁩```\n",
    ">\n",
    "> Change line 83 as per the diff:\n",
    ">\n",
    ">    ```python\n",
    ">    -  with np.load(path) as f:\n",
    ">    +  with np.load(path, allow_pickle=True) as f:\n",
    ">    ```\n",
    "> For alternative solutions, see <a>https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "num_of_words=10000  # keep the top 10,000 most frequently occurring words in the training data\n",
    "(x_train, y_train), (x_test, y_test) = data.load_data(num_words=num_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 8982, labels: 8982\n"
     ]
    }
   ],
   "source": [
    "print(\"Training entries: {}, labels: {}\".format(len(x_train), len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the first and second newswires\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(87, 56)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of words in the first and second newswires')\n",
    "len(x_train[0]), len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How the first newswire looks like:\n",
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print('How the first newswire looks like:')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing entries: 2246, labels: 2246\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing entries: {}, labels: {}\".format(len(x_test), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the integers back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = tf.keras.datasets.reuters.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> <UNK> <UNK> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider the first 256 words within the newswire\n",
    "max_newswire_length = 256\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_newswire_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_newswire_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the first and second newswires\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of words in the first and second newswires:')\n",
    "len(x_train[0]), len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded first newswire\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    1    2    2    8   43   10  447    5   25  207  270    5 3095\n",
      "  111   16  369  186   90   67    7   89    5   19  102    6   19  124\n",
      "   15   90   67   84   22  482   26    7   48    4   49    8  864   39\n",
      "  209  154    6  151    6   83   11   15   22  155   11   15    7   48\n",
      "    9 4579 1005  504    6  258    6  272   11   15   22  134   44   11\n",
      "   15   16    8  197 1245   90   67   52   29  209   30   32  132    6\n",
      "  109   15   17   12]\n"
     ]
    }
   ],
   "source": [
    "print('Padded first newswire:')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 256, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 377,846\n",
      "Trainable params: 377,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 0\n",
    "embedding_vecor_length = 32\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(input_dim=num_of_words,\n",
    "                                 output_dim=embedding_vecor_length,\n",
    "                                 input_length=max_newswire_length))\n",
    "model.add(keras.layers.LSTM(100))\n",
    "model.add(keras.layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',  # expects integer targets\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 256, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256, 64)           24832     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 380,846\n",
      "Trainable params: 380,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "model_1 = keras.models.Sequential()\n",
    "model_1.add(keras.layers.Embedding(input_dim=num_of_words, # vocabulary size\n",
    "                                   output_dim=32,\n",
    "                                   input_length=max_newswire_length)\n",
    "           )\n",
    "model_1.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "model_1.add(keras.layers.LSTM(64))\n",
    "    model_1.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',  # rmsprop Loss: 1.9, Accuracy: 0.5\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_y_train shape: (8982, 46)\n",
      "binary_y_test shape: (2246, 46)\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 256, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 256, 128)          82432     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 454,830\n",
      "Trainable params: 454,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "num_classes = np.max(y_train) + 1  # 46\n",
    "# For use with loss='categorical_crossentropy',\n",
    "binary_y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "binary_y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('binary_y_train shape:', binary_y_train.shape)\n",
    "print('binary_y_test shape:', binary_y_test.shape)\n",
    "\n",
    "model_2 = keras.models.Sequential()\n",
    "model_2.add(keras.layers.Embedding(input_dim=num_of_words, # vocabulary size\n",
    "                                   output_dim=32,\n",
    "                                   input_length=max_newswire_length)\n",
    "           )\n",
    "model_2.add(keras.layers.LSTM(128, return_sequences=True))\n",
    "model_2.add(keras.layers.Dropout(0.5))\n",
    "model_2.add(keras.layers.LSTM(64))\n",
    "model_2.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/4\n",
      "8083/8083 [==============================] - 122s 15ms/sample - loss: 1.9111 - acc: 0.5135 - val_loss: 1.8403 - val_acc: 0.5284\n",
      "Epoch 2/4\n",
      "8083/8083 [==============================] - 101s 13ms/sample - loss: 1.7386 - acc: 0.5539 - val_loss: 1.7011 - val_acc: 0.5806\n",
      "Epoch 3/4\n",
      "8083/8083 [==============================] - 97s 12ms/sample - loss: 1.6496 - acc: 0.5818 - val_loss: 1.6376 - val_acc: 0.5918\n",
      "Epoch 4/4\n",
      "8083/8083 [==============================] - 99s 12ms/sample - loss: 1.5311 - acc: 0.6119 - val_loss: 1.6016 - val_acc: 0.6085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb352c40b8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 0\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=4,\n",
    "          batch_size=32,\n",
    "          validation_split=0.1,  # Do not use test data for validation\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/4\n",
      "8083/8083 [==============================] - 98s 12ms/sample - loss: 1.7667 - acc: 0.5207 - val_loss: 1.8845 - val_acc: 0.4994\n",
      "Epoch 2/4\n",
      "8083/8083 [==============================] - 106s 13ms/sample - loss: 1.7099 - acc: 0.5394 - val_loss: 1.7892 - val_acc: 0.5250\n",
      "Epoch 3/4\n",
      "8083/8083 [==============================] - 117s 14ms/sample - loss: 1.5948 - acc: 0.5710 - val_loss: 1.7560 - val_acc: 0.5417\n",
      "Epoch 4/4\n",
      "8083/8083 [==============================] - 124s 15ms/sample - loss: 1.5135 - acc: 0.5954 - val_loss: 1.6809 - val_acc: 0.5684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb3af5c128>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1\n",
    "model_1.fit(x_train,\n",
    "            y_train,\n",
    "            epochs=4,\n",
    "            batch_size=64,\n",
    "            validation_split=0.1,  # Do not use test data for validation\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/4\n",
      "8083/8083 [==============================] - 169s 21ms/sample - loss: 2.5440 - acc: 0.2735 - val_loss: 2.4811 - val_acc: 0.3315\n",
      "Epoch 2/4\n",
      "8083/8083 [==============================] - 177s 22ms/sample - loss: 2.4179 - acc: 0.3540 - val_loss: 2.4855 - val_acc: 0.3315\n",
      "Epoch 3/4\n",
      "8083/8083 [==============================] - 178s 22ms/sample - loss: 2.4121 - acc: 0.3540 - val_loss: 2.4696 - val_acc: 0.3315\n",
      "Epoch 4/4\n",
      "8083/8083 [==============================] - 177s 22ms/sample - loss: 2.4033 - acc: 0.3540 - val_loss: 2.4680 - val_acc: 0.3315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb3da00400>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2\n",
    "model_2.fit(x_train,\n",
    "            binary_y_train,\n",
    "            epochs=4,\n",
    "            batch_size=64,\n",
    "            validation_split=0.1,  # Do not use test data for validation\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 5s 2ms/sample - loss: 1.6142 - acc: 0.5980\n",
      "Model 0:\n",
      "Loss: 1.6141657850515174, Accuracy: 0.5979518890380859\n"
     ]
    }
   ],
   "source": [
    "# Use the test data only once for the final evaluation\n",
    "model0_res = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Model 0:\\nLoss: {}, Accuracy: {}'.format(*model0_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 9s 4ms/sample - loss: 1.9049 - acc: 0.5067\n",
      "Model 1:\n",
      "Loss: 1.904869069186896, Accuracy: 0.5066785216331482\n"
     ]
    }
   ],
   "source": [
    "model_1_res = model_1.evaluate(x_test, y_test)\n",
    "\n",
    "print('Model 1:\\nLoss: {}, Accuracy: {}'.format(*model_1_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 12s 5ms/sample - loss: 2.4172 - acc: 0.3620\n",
      "Model 2:\n",
      "Loss: 2.417198654380316, Accuracy: 0.36197686195373535\n"
     ]
    }
   ],
   "source": [
    "model_2_res = model_2.evaluate(x_test, binary_y_test)\n",
    "\n",
    "print('Model 2:\\nLoss: {}, Accuracy: {}'.format(*model_2_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "I have built 3 models with arbitrarily chosen parameters. The simplest model of roughy the following form:\n",
    "\n",
    "\n",
    "         +-------------------+   +----------------+   +------------+\n",
    "    x -->| Embedding         |-->| LSTM (100)     |-->| DenseLayer |--> y\n",
    "         | (out_shape=256,32)|   |                |   | (softmax)  |\n",
    "         +-------------------+   +----------------+   +------------+\n",
    "\n",
    "with `loss_function='sparse_categorical_crossentropy'`, `optimizer='adam'`, `batch_size=32`, and `epochs=4`, has <u><b>accuracy</b></u> of approximately <u><b>0.6</b></u>.\n",
    "\n",
    "\n",
    "Then I made random modifications to this model to produce two more models of the following forms:\n",
    "\n",
    "         +-------------------+   +----------------+   +----------------+   +------------+\n",
    "    x -->| Embedding         |-->| LSTM (256, 64) |-->| LSTM (64)      |-->| DenseLayer |--> y\n",
    "         | (out_shape=256,32)|   |                |   |                |   | (sigmoid)  |\n",
    "         +-------------------+   +----------------+   +----------------+   +------------+\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "         +-------------------+   +----------------+   +----------------+   +------------+\n",
    "    x -->| Embedding         |-->| LSTM (256, 128)|-->| LSTM (64)      |-->| DenseLayer |--> y\n",
    "         | (out_shape=256,32)|   | Dropout (0.5)  |   |                |   | (sigmoid)  |\n",
    "         +-------------------+   +----------------+   +----------------+   +------------+\n",
    "\n",
    "which have the accuracy scores of 0.5 and 0.4, respectively.\n",
    "\n",
    "My conclusion is that the simplest model works best. It is probably possible to improve it but not by randomly modifying its parameters without experience and intuition about what might work better. A more fruitful approach might be to use grid search for parameters but it would take an enormous amount of time so I decided not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
