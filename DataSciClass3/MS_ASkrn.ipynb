{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestones 1 & 2 & 3<a id='top'></a>\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "SECOM manufacturing Data Set from the UCI Machine Learning Repository\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/secom/\n",
    "\n",
    "The dataset consists of two files:\n",
    "1. a dataset file <a href='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'>SECOM</a> containing 1567 examples, each with 591 features, 104 fails\n",
    "2. a <a href='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'>labels</a> file listing the classifications and date time stamp for each example\n",
    "\n",
    "The dataset represents a selection of features where \n",
    "each example represents a single production entity with associated measured \n",
    "features and the labels represent a simple pass/fail yield for in house line \n",
    "testing and associated date time stamp.\n",
    "\n",
    "-1 corresponds to a pass and 1 corresponds to a fail and the data time stamp is for that specific test \n",
    "point.\n",
    "\n",
    "Null values are represented by the 'NaN' value.\n",
    "\n",
    "## Milestone 1 Instructions\n",
    "\n",
    "Assume the 591 attributes represent different sensors readings across the manufacturing process. \n",
    "\n",
    "Using the <a href='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'>SECOM dataset</a>, create a new notebook, and perform each of the following tasks and answer the related questions:\n",
    "\n",
    "1. Refine your data understanding and prepare a DFD of solving the manufacturing quality problem\n",
    "2. Read and merge data\n",
    "3. Clean and prepare data \n",
    "4. Visually explore data\n",
    "5. Handle class imbalance problem\n",
    "6. Apply feature selection techniques to reduce dimensionality of data\n",
    "7. What are your initial findings? Describe them in a [summary](#summary1) section\n",
    "\n",
    "## Milestone 2 Instructions\n",
    "\n",
    "1. Split prepared data from Milestone 1 into training and testing\n",
    "2. Build a <b>decision tree</b> model that detects faulty products\n",
    "3. Build an <b>ensemble model</b> that detects faulty products\n",
    "4. Build an <b>SVM</b> model\n",
    "5. Evaluate all three models\n",
    "6. Describe your [findings](#summary2)\n",
    "\n",
    "## Milestone 3 Instructions\n",
    "\n",
    "1. Build a simple neural networks model - see the [SLP section](#slp) \n",
    "2. Build a DNN model - see the [MLP section](#mlp)\n",
    "3. Build a RNN model - see the [RNN section](#rnn)\n",
    "4. Summarize your findings with examples. Explain what the manufacturer should focus on to optimize the manufacturing process - see the [Summary section](#summary3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to run\n",
    "# !pip install mlxtend\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE  # Recursive Feature Elimination\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flow Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PFD](pfd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the data set from a local file if it exists...\n",
      "\tData loaded from local file\n",
      "Trying to load the data set from a local file if it exists...\n",
      "\tData loaded from local file\n"
     ]
    }
   ],
   "source": [
    "url_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "url_labels = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "\n",
    "local_path_data = './secom.data'\n",
    "local_path_labels = './secom_labels.data'\n",
    "\n",
    "def load_data(a_url, path):\n",
    "    \"\"\"Return a DataFrame containing a dataset from url or local path.\"\"\"\n",
    "    print('Trying to load the data set from a local file if it exists...')\n",
    "    try:\n",
    "        data_set = pd.read_csv(path, sep = ' ', header = None)\n",
    "    except FileNotFoundError:\n",
    "        print('\\tFile not found, loading from Internet...')\n",
    "        data_set = pd.read_csv(a_url, sep = ' ', header = None)\n",
    "        print('\\t\\tData loaded from url')\n",
    "    else:\n",
    "        print('\\tData loaded from local file')\n",
    "    \n",
    "    return data_set\n",
    "\n",
    "data = load_data(url_data, local_path_data)\n",
    "labels = load_data(url_labels, local_path_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 590)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.006</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1          2          3       4      5         6       7    \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "\n",
       "      8       9    ...    580       581     582     583     584     585  \\\n",
       "0  1.5005  0.0162  ...    NaN       NaN  0.5005  0.0118  0.0035  2.3630   \n",
       "1  1.4966 -0.0005  ...  0.006  208.2045  0.5019  0.0223  0.0055  4.4447   \n",
       "\n",
       "      586     587    588       589  \n",
       "0     NaN     NaN    NaN       NaN  \n",
       "1  0.0096  0.0201  0.006  208.2045  \n",
       "\n",
       "[2 rows x 590 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 11:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 12:32:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                    1\n",
       "0 -1  19/07/2008 11:55:00\n",
       "1 -1  19/07/2008 12:32:00"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "labels.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Merge data and labels</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataframe, rows by columns:\n",
      "(1567, 591)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1          2          3       4      5         6       7  \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "2  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
       "\n",
       "        8       9  ...       581     582     583     584     585     586  \\\n",
       "0  1.5005  0.0162  ...       NaN  0.5005  0.0118  0.0035  2.3630     NaN   \n",
       "1  1.4966 -0.0005  ...  208.2045  0.5019  0.0223  0.0055  4.4447  0.0096   \n",
       "2  1.4436  0.0041  ...   82.8602  0.4958  0.0157  0.0039  3.1745  0.0584   \n",
       "\n",
       "      587     588       589  outcome  \n",
       "0     NaN     NaN       NaN       -1  \n",
       "1  0.0201  0.0060  208.2045       -1  \n",
       "2  0.0484  0.0148   82.8602        1  \n",
       "\n",
       "[3 rows x 591 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production_df = data\n",
    "# production_df['timestamp'] = labels[1]  # DO I NEED THE TIMESTAMP?\n",
    "production_df['outcome'] = labels[0]\n",
    "print('The size of the dataframe, rows by columns:')\n",
    "print(production_df.shape)\n",
    "production_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Split the dataset 80/20 into:</b> \n",
    "\n",
    "| 80% of records        | 20% of records      |\n",
    "|-----------------------|---------------------|\n",
    "| training data X dataframe | test data XX dataframe |\n",
    "| training label Y array   | test label YY array |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prod = production_df.loc[:, 'outcome'].values.astype(str)\n",
    "x_prod = production_df.drop('outcome', axis = 1)\n",
    "X, XX, Y, YY = train_test_split(x_prod,\n",
    "                                y_prod,\n",
    "                                train_size = 0.8,\n",
    "                                test_size = 0.2,\n",
    "                                random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace -1 (indicating a pass) with 0 in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY THIS\n",
    "# tdf['Sex'] = tdf['Sex'].map({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of pass/fail labels in the training set\n",
      "Pass labels -1: 1162 cases\n",
      "Fail labels 1: 91 cases\n"
     ]
    }
   ],
   "source": [
    "# Distribution of positive and negative cases in the train and test data\n",
    "Y = Y.astype(int)\n",
    "mask_neg_outcome = (Y == -1)  # --> bool array\n",
    "pos_outcome  = (Y == 1)\n",
    "print('Distribution of pass/fail labels in the training set')\n",
    "print('Pass labels -1:', sum(mask_neg_outcome), 'cases')\n",
    "print('Fail labels 1:', sum(pos_outcome), 'cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass labels 0: 1162 cases\n",
      "Fail labels 1: 91 cases\n"
     ]
    }
   ],
   "source": [
    "# Replace -1 with 0 in the train label Y array\n",
    "Y[mask_neg_outcome] = 0\n",
    "zero_outcome = (Y == 0)\n",
    "pos_outcome  = (Y == 1)\n",
    "print('Pass labels 0:', sum(zero_outcome), 'cases')\n",
    "print('Fail labels 1:', sum(pos_outcome), 'cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of pass/fail labels in the testing set\n",
      "Pass labels -1: 301 cases\n",
      "Fail labels 1: 13 cases\n"
     ]
    }
   ],
   "source": [
    "YY = YY.astype(int)\n",
    "mask_neg_outcome = (YY == -1)  # --> bool array\n",
    "pos_outcome  = (YY == 1)\n",
    "print('Distribution of pass/fail labels in the testing set')\n",
    "print('Pass labels -1:', sum(mask_neg_outcome), 'cases')\n",
    "print('Fail labels 1:', sum(pos_outcome), 'cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass labels 0: 301 cases\n",
      "Fail labels 1: 13 cases\n"
     ]
    }
   ],
   "source": [
    "# Replace -1 with 0 in the test label YY array\n",
    "YY[mask_neg_outcome] = 0\n",
    "zero_outcome = (YY == 0)\n",
    "pos_outcome  = (YY == 1)\n",
    "print('Pass labels 0:', sum(zero_outcome), 'cases')\n",
    "print('Fail labels 1:', sum(pos_outcome), 'cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to drop constant-valued columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the function\n",
      "REMOVING CONSTANT COLUMNS\n",
      "List of constant-valued columns to be removed:\n",
      "Index(['a', 'b'], dtype='object')\n",
      "Training set shape BEFORE dropping constant columns: (3, 3)\n",
      "Training set shape AFTER dropping constant columns: (3, 1)\n",
      "Testing set shape BEFORE dropping constant columns: (3, 3)\n",
      "Testing set shape AFTER dropping constant columns: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "def drop_constant_columns(training_set_X, testing_set_XX):\n",
    "    \"\"\"Remove constant columns and return two updated datasets.\"\"\"\n",
    "    print('REMOVING CONSTANT COLUMNS')\n",
    "    std_filter = (training_set_X.std() == 0)  # Select columns for which st. dev. is 0\n",
    "    constant_column_names = std_filter[std_filter].index\n",
    "    print('List of constant-valued columns to be removed:')\n",
    "    print(constant_column_names)\n",
    "\n",
    "    # Drop constant columns in the traing set X\n",
    "    print('Training set shape BEFORE dropping constant columns:', training_set_X.shape)\n",
    "    training_set_X = training_set_X.drop(constant_column_names, axis = 1)\n",
    "    print('Training set shape AFTER dropping constant columns:', training_set_X.shape)\n",
    "\n",
    "    # Drop the same columns in the testing set XX\n",
    "    print('Testing set shape BEFORE dropping constant columns:', testing_set_XX.shape)\n",
    "    testing_set_XX = testing_set_XX.drop(constant_column_names, axis = 1)\n",
    "    print('Testing set shape AFTER dropping constant columns:', testing_set_XX.shape)\n",
    "    \n",
    "    return training_set_X, testing_set_XX\n",
    "\n",
    "# Function test\n",
    "print('Testing the function')\n",
    "test_dict = {'a': pd.Series([1, 1, 1]),\n",
    "          'b': pd.Series([0.5, float('nan'), 0.5]),\n",
    "          'c': pd.Series([0.1, 0.2, 0.2])}\n",
    "test_df = pd.DataFrame(test_dict)\n",
    "returned_df_one, returned_df_two = drop_constant_columns(test_df, test_df)\n",
    "assert returned_df_one.shape == (3, 1)\n",
    "assert returned_df_two.shape == (3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with missing values and outliers? \n",
    "\n",
    "For outliers:\n",
    "- Impute outlier values for the training set -- replace these values with a guess: median\n",
    "- Use the same median values to make replacements for outliers in the corresponding columns in the testing set\n",
    "- Check remaining columns in the testing set to see if there are any outliers there and replace them with medians\n",
    "\n",
    "For missing values:\n",
    "- Remove columns with more than half values missing\n",
    "- Impute other missing values using medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41951 missing values in both the training and testing data sets\n"
     ]
    }
   ],
   "source": [
    "miss_val_training = sum(X.isnull().sum())\n",
    "miss_val_testing = sum(XX.isnull().sum())\n",
    "ttl_missing = miss_val_training + miss_val_testing\n",
    "print('There are {} missing values in both the training and testing data sets'.format(ttl_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which columns in the training set have more than half values missing?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([ 72,  73,  85, 109, 110, 111, 157, 158, 220, 244, 245, 246, 292, 293,\n",
       "       345, 346, 358, 382, 383, 384, 492, 516, 517, 518, 578, 579, 580, 581],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Which columns in the training set have more than half values missing?')\n",
    "filter_nan = (X.isna().sum() > X.shape[0]/2)\n",
    "col_to_remove = X.columns[filter_nan]\n",
    "col_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will remove these columns from both the training and testing sets.\n",
      "X shape before: (1253, 590)\n",
      "X shape after: (1253, 562)\n",
      "XX shape before: (314, 590)\n",
      "X shape after: (314, 562)\n"
     ]
    }
   ],
   "source": [
    "print('I will remove these columns from both the training and testing sets.')\n",
    "# col_to_remove = \n",
    "print('X shape before:', X.shape)\n",
    "X = X.drop(col_to_remove, axis = 1)\n",
    "print('X shape after:', X.shape)\n",
    "print('XX shape before:', XX.shape)\n",
    "XX = XX.drop(col_to_remove, axis = 1)\n",
    "print('X shape after:', XX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That removed 30268 missing values, with 11683 remaining missing values to be imputed\n"
     ]
    }
   ],
   "source": [
    "miss_val_training = sum(X.isnull().sum())\n",
    "miss_val_testing = sum(XX.isnull().sum())\n",
    "updated_ttl_missing = miss_val_training + miss_val_testing\n",
    "diff = ttl_missing - updated_ttl_missing\n",
    "print('That removed {} missing values, with {} remaining missing values to be imputed'\n",
    "      .format(diff, updated_ttl_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to impute outliers and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the function\n",
      "IMPUTING OUTLIERS\n",
      "Done\n",
      "IMPUTING OUTLIERS\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def impute_outliers(a_dataframe, col_value_pairs_dict = None):\n",
    "    \"\"\"Return a dataset with outliers replaced by imputed values.\"\"\"\n",
    "    print('IMPUTING OUTLIERS')\n",
    "    col_value_pairs = {}  # {column_index_#: imputed_replacement_value}\n",
    "    a_dataframe = a_dataframe.copy()\n",
    "    for column_name in a_dataframe.dtypes.keys():\n",
    "        try:\n",
    "            column_data = a_dataframe.loc[:, column_name]\n",
    "            mean = np.nanmean(column_data)  # non-numeric columns will be skipped\n",
    "        except TypeError:\n",
    "            pass\n",
    "        else:\n",
    "            st_dev = np.nanstd(column_data)\n",
    "            limit_hi = mean + 2 * st_dev\n",
    "            limit_lo = mean - 2 * st_dev\n",
    "            # True for outliers, False if within limits\n",
    "            flag_bad = (column_data < limit_lo) | (column_data > limit_hi)\n",
    "            replacement_median = np.nanmedian(column_data)\n",
    "            if col_value_pairs_dict:  # Try to replace from dict first\n",
    "                try:\n",
    "                    column_data[flag_bad] = col_value_pairs_dict[column_name]\n",
    "                except KeyError:\n",
    "                    column_data[flag_bad] = replacement_median\n",
    "            else:\n",
    "                column_data[flag_bad] = replacement_median\n",
    "            a_dataframe[column_name] = column_data\n",
    "            # Keep column # and replacement value\n",
    "            col_value_pairs[column_name] = replacement_median\n",
    "    print('Done')\n",
    "    return a_dataframe, col_value_pairs\n",
    "\n",
    "# Function test\n",
    "print('Testing the function')\n",
    "test_dict = {'a': [71, 70, 73, 70, 70, 69, 70, 72, float('nan'), 300, 71, 69]}\n",
    "test_df = pd.DataFrame(test_dict)\n",
    "expected_median = 70.0  # np.nanmedian(test_dict['a']) --> 70.0\n",
    "# Replace outlier from data alone or replace from the provided dict with col name & value\n",
    "arguments = ((test_df, None), (test_df, {'a': 70.0}))\n",
    "for arg in arguments:\n",
    "    returned_data, returned_dict = impute_outliers(*arg)\n",
    "    assert returned_data['a'][9] == expected_median\n",
    "    assert returned_dict['a'] == expected_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the function\n",
      "IMPUTING MISSING VALUES\n",
      "IMPUTING MISSING VALUES\n"
     ]
    }
   ],
   "source": [
    "def impute_missing(a_dataframe, fill_value):\n",
    "    \"\"\"Return a dataset with NaN values replaced by values from list.\"\"\"\n",
    "    print('IMPUTING MISSING VALUES')\n",
    "    a_dataframe = a_dataframe.copy()\n",
    "    for fill_val, column_name in zip(fill_value, a_dataframe.dtypes.keys()):\n",
    "        try:\n",
    "            column_data = a_dataframe.loc[:, column_name]\n",
    "            median = np.nanmedian(column_data)  # non-numeric columns will be skipped\n",
    "        except TypeError:\n",
    "            pass\n",
    "        else:\n",
    "            # True for NaN, False otherwise\n",
    "            has_nan = np.isnan(column_data)\n",
    "            column_data[has_nan] = fill_val\n",
    "            a_dataframe[column_name] = column_data\n",
    "\n",
    "    return a_dataframe\n",
    "\n",
    "# Test function\n",
    "print('Testing the function')\n",
    "test_dict = {'a': [71, 70, 73, 70, 70, 69, 70, 72, 71, float('nan'), 71, 69]}\n",
    "test_df = pd.DataFrame(test_dict)\n",
    "expected_median = 70.0  # np.nanmedian(test_dict['a']) --> 70.0\n",
    "assert impute_missing(test_df, [expected_median])['a'][9] == expected_median\n",
    "assert isinstance(impute_missing(test_df, [expected_median]), pd.core.frame.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to drop almost constant-valued columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_quasi_constant_columns(X_training, XX_testing):\n",
    "    \"\"\"Return datasets with almost constant columns removed.\"\"\"\n",
    "    print('DROPPING ALMOST CONSTANT-VALUED COLUMNS')\n",
    "    # Use VarianceThreshold method from sklearn.feature_selection\n",
    "    # Drop features whose variance is below a threshold of 0.01\n",
    "    th = 0.01\n",
    "    quasi_constant_filter = VarianceThreshold(threshold = th)\n",
    "    quasi_constant_filter.fit(X_training)\n",
    "    # A list of almost constant features\n",
    "    quasi_constant_columns = [column for column in X_training.columns  \n",
    "                        if column not in X_training.columns[quasi_constant_filter.get_support()]]\n",
    "    print('Number of features below the threshold of {} is {}'\n",
    "          .format(th, len(quasi_constant_columns)))\n",
    "    print('List of almost constant features:')\n",
    "    print(quasi_constant_columns)\n",
    "    # Drop almost constant features from both the training and testing sets\n",
    "    X_training = X_training.drop(quasi_constant_columns, axis = 1)\n",
    "    XX_testing = XX_testing.drop(quasi_constant_columns, axis = 1)\n",
    "    print('Training set shape after dropping almost constant columns:',\n",
    "          X_training.shape)\n",
    "    print('Testing set shape after dropping almost constant columns:',\n",
    "          XX_testing.shape)\n",
    "    return X_training, XX_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with highly correlated features?\n",
    "- For the training set:\n",
    "    - Find pairs of highly correlated features\n",
    "    - In each pair, keep one feature that is more correlated with the target variable\n",
    "- Then drop the same features in the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to drop linearly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_lower_corr(X_dataframe, Y_dataframe, a_tuple):\n",
    "    \"\"\"Return one of two column names in a_tuple with lower corr.coef with Y label.\"\"\"\n",
    "    col_num0 = a_tuple[0]\n",
    "    col_num1 = a_tuple[1]\n",
    "    coef0 = np.corrcoef(X_dataframe[col_num0], Y_dataframe)[0][1]\n",
    "    coef1 = np.corrcoef(X_dataframe[col_num1], Y_dataframe)[0][1]\n",
    "    if coef0 <= coef1:\n",
    "        return col_num0\n",
    "    return col_num1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_lin_corr_columns(training_set_X, testing_set_XX):\n",
    "    \"\"\"Return two datasets with linearly correlated features removed.\"\"\"\n",
    "    # Use a correlation matrix to identify highly correlated features\n",
    "    correlation_matrix = training_set_X.corr()\n",
    "    # Use a transpose of the corr. matrix to extract column names that a given column is correlated with\n",
    "    correlation_matrix_T = correlation_matrix.T\n",
    "    correlated_feature_pairs = []\n",
    "    threshold_corrcoef = 0.8 # the choice is arbitrary, just closer to 1\n",
    "    for i in range(len(correlation_matrix.columns)):  \n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold_corrcoef:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_feature_pairs.append((correlation_matrix_T.columns[j],\n",
    "                                                 correlation_matrix.columns[i]))\n",
    "    print('Number of pairs of correlated features identified:',\n",
    "          len(correlated_feature_pairs))\n",
    "    print('First three pairs:', correlated_feature_pairs[:3])\n",
    "    \n",
    "    corr_columns_to_drop = set()\n",
    "    for feature_pair in correlated_feature_pairs:\n",
    "        corr_columns_to_drop.add(choose_lower_corr(training_set_X, Y, feature_pair))\n",
    "        \n",
    "    print('The first pair of columns in <correlated_feature_pairs> list:',\n",
    "          correlated_feature_pairs[0])\n",
    "    print('Their corr. coef. is:',\n",
    "          np.corrcoef(training_set_X[correlated_feature_pairs[0][0]],\n",
    "                      training_set_X[correlated_feature_pairs[0][1]])[0][1])\n",
    "    print('{} and label Y corr. coef.: {}'\n",
    "          .format(correlated_feature_pairs[0][0],\n",
    "                  np.corrcoef(training_set_X[correlated_feature_pairs[0][0]], Y)[0][1]))\n",
    "    print('{} and label Y corr. coef. {}:'\n",
    "          .format(correlated_feature_pairs[0][1],\n",
    "                  np.corrcoef(training_set_X[correlated_feature_pairs[0][1]], Y)[0][1]))\n",
    "    col_to_drop = choose_lower_corr(training_set_X, Y, correlated_feature_pairs[0])\n",
    "    print('Function choose_lower_corr() returns', col_to_drop, 'which will be dropped')\n",
    "    \n",
    "    print('Number and names of correlated features which will be removed:')\n",
    "    print(len(corr_columns_to_drop))\n",
    "    print(sorted(list(corr_columns_to_drop)))\n",
    "    \n",
    "    # Drop linearly correlated columns\n",
    "    training_set_X = training_set_X.drop(corr_columns_to_drop, axis = 1)\n",
    "    print('Training set shape after removing correlated features:',\n",
    "          training_set_X.shape)\n",
    "    testing_set_XX = testing_set_XX.drop(corr_columns_to_drop, axis = 1)\n",
    "    print('Testing set shape after removing correlated features:',\n",
    "          testing_set_XX.shape)\n",
    "    \n",
    "    return training_set_X, testing_set_XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to drop columns with a high mutual information score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_MI(x, y, bins):\n",
    "    \"\"\"Return a mutual information score for two features.\"\"\"\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_lower_mi(X_dataframe, Y_data, a_tuple):\n",
    "    \"\"\"Return one of two column names in a_tuple with a lower MI score with Y label.\"\"\"\n",
    "    col_num0 = a_tuple[0]\n",
    "    col_num1 = a_tuple[1]\n",
    "    mi0 = calc_MI(X_dataframe[col_num0], Y_data, 20)\n",
    "    mi1 = calc_MI(X_dataframe[col_num1], Y_data, 20)\n",
    "    if mi0 <= mi1:\n",
    "        return col_num0\n",
    "    return col_num1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_high_mi_pairs(training_set_X, testing_set_XX):\n",
    "    \"\"\"Return a list of pairs of columns with a high mutual information score.\"\"\"\n",
    "    current_columns = list(training_set_X.columns)\n",
    "    # All possible combinations of columns by using itertools\n",
    "    column_combinations = combinations(current_columns, 2)\n",
    "    # List of columns with high mutual information score - I chose 0.9 as a threshold\n",
    "    mutual_info_columns_pairs = []\n",
    "    bins = 20\n",
    "    for column_pair in column_combinations:\n",
    "        mi = calc_MI(training_set_X[column_pair[0]], \n",
    "                     training_set_X[column_pair[1]],\n",
    "                     bins)\n",
    "        if mi > 0.9:\n",
    "            mutual_info_columns_pairs.append((column_pair[0], column_pair[1]))\n",
    "    return mutual_info_columns_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_mi_columns(training_set_X, testing_set_XX):\n",
    "    \"\"\"Return two datasets with removed columns with a high mutual information score.\"\"\"\n",
    "    mutual_info_columns_pairs = calc_high_mi_pairs(training_set_X, testing_set_XX)\n",
    "    print('Number of pairs of features with high mutual information score identified:')\n",
    "    print(len(mutual_info_columns_pairs))\n",
    "    print(mutual_info_columns_pairs)\n",
    "    mi_based_columns_to_drop = set()\n",
    "    for feature_pair in mutual_info_columns_pairs:\n",
    "        mi_based_columns_to_drop.add(choose_lower_mi(training_set_X, Y, feature_pair))\n",
    "    print('Number and names of features with a high MI score to be removed:')\n",
    "    print(len(mi_based_columns_to_drop))\n",
    "    print(mi_based_columns_to_drop)\n",
    "    \n",
    "    # Drop columns with high mutual information score\n",
    "    training_set_X = training_set_X.drop(mi_based_columns_to_drop, axis = 1)\n",
    "    print('Training set shape after removing features with a high mutual correlation score',\n",
    "          training_set_X.shape)\n",
    "    testing_set_XX = testing_set_XX.drop(mi_based_columns_to_drop, axis = 1)\n",
    "    print('Testing set shape after removing features with a high mutual correlation score',\n",
    "          testing_set_XX.shape)\n",
    "    \n",
    "    return training_set_X, testing_set_XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that calls all data preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(transformations_list, training_set_X, testing_set_XX):\n",
    "    \"\"\"Define a function that calls all data preparate functions in the transformation list.\"\"\"\n",
    "    for transformation in transformations_list:\n",
    "        if transformation == 'drop_constant_columns':\n",
    "            training_set_X, testing_set_XX = drop_constant_columns(training_set_X,\n",
    "                                                                   testing_set_XX)\n",
    "        if transformation == 'impute_outliers':\n",
    "            training_set_X, replacement_values = impute_outliers(training_set_X)\n",
    "            testing_set_XX, _ = impute_outliers(testing_set_XX,\n",
    "                                                 replacement_values)\n",
    "        if transformation == 'impute_missing':\n",
    "            fill_val = training_set_X.median()  # replacement value\n",
    "            print('Number of missing values in training set X:',\n",
    "                  sum(training_set_X.isnull().sum()))\n",
    "            training_set_X = impute_missing(training_set_X, fill_val)\n",
    "            print('Number of missing values in training set X:',\n",
    "                  sum(training_set_X.isnull().sum()))\n",
    "            print('Number of missing values in testing set XX:',\n",
    "                  sum(testing_set_XX.isnull().sum()))\n",
    "            testing_set_XX = impute_missing(testing_set_XX, fill_val)\n",
    "            print('Number of missing values in testing set XX:',\n",
    "                  sum(testing_set_XX.isnull().sum()))\n",
    "        if transformation == 'drop_quasi_constant_columns':\n",
    "            training_set_X, testing_set_XX = drop_quasi_constant_columns(training_set_X,\n",
    "                                                                         testing_set_XX)\n",
    "        if transformation == 'drop_lin_corr_columns':\n",
    "            training_set_X, testing_set_XX = drop_lin_corr_columns(training_set_X,\n",
    "                                                                   testing_set_XX)\n",
    "        if transformation == 'drop_high_mi_columns':\n",
    "            training_set_X, testing_set_XX = drop_high_mi_columns(training_set_X,\n",
    "                                                                   testing_set_XX)\n",
    "            \n",
    "    return training_set_X, testing_set_XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant columns, impute outliers, impute missing values, remove almost constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING CONSTANT COLUMNS\n",
      "List of constant-valued columns to be removed:\n",
      "Index([  5,  13,  42,  49,  52,  69,  97, 141, 149, 178,\n",
      "       ...\n",
      "       529, 530, 531, 532, 533, 534, 535, 536, 537, 538],\n",
      "      dtype='object', length=116)\n",
      "Training set shape BEFORE dropping constant columns: (1253, 562)\n",
      "Training set shape AFTER dropping constant columns: (1253, 446)\n",
      "Testing set shape BEFORE dropping constant columns: (314, 562)\n",
      "Testing set shape AFTER dropping constant columns: (314, 446)\n",
      "IMPUTING OUTLIERS\n",
      "Done\n",
      "IMPUTING OUTLIERS\n",
      "Done\n",
      "Number of missing values in training set X: 8662\n",
      "IMPUTING MISSING VALUES\n",
      "Number of missing values in training set X: 0\n",
      "Number of missing values in testing set XX: 2206\n",
      "IMPUTING MISSING VALUES\n",
      "Number of missing values in testing set XX: 0\n",
      "DROPPING ALMOST CONSTANT-VALUED COLUMNS\n",
      "Number of features below the threshold of 0.01 is 171\n",
      "List of almost constant features:\n",
      "[7, 8, 9, 10, 11, 17, 19, 20, 26, 30, 53, 54, 56, 57, 58, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 86, 87, 89, 91, 92, 93, 94, 95, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 112, 113, 114, 116, 118, 119, 120, 121, 123, 124, 130, 131, 132, 143, 144, 145, 146, 147, 153, 156, 163, 164, 165, 168, 171, 172, 173, 174, 175, 176, 184, 195, 206, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 221, 222, 224, 227, 228, 238, 239, 247, 248, 249, 251, 253, 254, 267, 278, 279, 280, 281, 282, 288, 290, 291, 298, 299, 300, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 317, 320, 331, 334, 340, 342, 347, 348, 349, 350, 351, 352, 353, 354, 355, 357, 359, 360, 362, 365, 366, 367, 368, 376, 377, 385, 386, 387, 389, 391, 392, 393, 405, 447, 478, 542, 543, 544, 558, 560, 563, 565, 567, 575, 582, 583, 584, 586, 587, 588]\n",
      "Training set shape after dropping almost constant columns: (1253, 275)\n",
      "Testing set shape after dropping almost constant columns: (314, 275)\n"
     ]
    }
   ],
   "source": [
    "X, XX = prepare_data(['drop_constant_columns',\n",
    "                      'impute_outliers',\n",
    "                      'impute_missing',\n",
    "                      'drop_quasi_constant_columns'],\n",
    "                     X,\n",
    "                     XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove linearly correlated columns and columns with a high mutual information score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I select pairs of columns which have a correlation coefficient above 0.8 and pairs of columns which have a mutual information score above 0.9. Then I compare each column in the pairs to the target variable Y and remove that column which has a smaller impact on Y (either a smaller correlation coefficient or a smaller mutual information score, respectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs of correlated features identified: 148\n",
      "First three pairs: [(12, 18), (21, 25), (21, 27)]\n",
      "The first pair of columns in <correlated_feature_pairs> list: (12, 18)\n",
      "Their corr. coef. is: 0.9221783717196761\n",
      "12 and label Y corr. coef.: -0.010873669258071251\n",
      "18 and label Y corr. coef. -0.02293316971370265:\n",
      "Function choose_lower_corr() returns 18 which will be dropped\n",
      "Number and names of correlated features which will be removed:\n",
      "98\n",
      "[18, 25, 27, 31, 34, 43, 50, 64, 70, 127, 135, 136, 137, 138, 139, 142, 148, 150, 151, 152, 154, 159, 160, 161, 166, 167, 169, 177, 182, 183, 185, 187, 196, 197, 198, 199, 202, 203, 205, 207, 218, 223, 225, 252, 255, 269, 270, 273, 275, 283, 286, 287, 289, 297, 301, 318, 321, 323, 324, 332, 335, 337, 338, 339, 341, 344, 356, 363, 388, 390, 406, 409, 410, 413, 415, 427, 428, 430, 438, 441, 445, 452, 453, 455, 469, 479, 495, 522, 540, 541, 549, 550, 551, 553, 557, 561, 564, 566]\n",
      "Training set shape after removing correlated features: (1253, 177)\n",
      "Testing set shape after removing correlated features: (314, 177)\n",
      "Number of pairs of features with high mutual information score identified:\n",
      "4\n",
      "[(295, 431), (296, 432), (552, 555), (573, 577)]\n",
      "Number and names of features with a high MI score to be removed:\n",
      "4\n",
      "{432, 577, 555, 431}\n",
      "Training set shape after removing features with a high mutual correlation score (1253, 173)\n",
      "Testing set shape after removing features with a high mutual correlation score (314, 173)\n"
     ]
    }
   ],
   "source": [
    "X, XX = prepare_data(['drop_lin_corr_columns',\n",
    "                      'drop_high_mi_columns'],\n",
    "                     X,\n",
    "                     XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply wrapper methods of feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commented out and hardcoded the results to save on the running time\n",
    "# # Forward Stepwise Feature Selection\n",
    "# lr = linear_model.LinearRegression()\n",
    "\n",
    "# sfs = SFS(lr, \n",
    "#           k_features = X.shape[1], # k_features has to be smaller or equal to the number of features. \n",
    "#                             # If equal to, it starts from\n",
    "#                             # intercept to the full model\n",
    "#           forward = True,\n",
    "#           floating = False, \n",
    "#           scoring = 'neg_mean_squared_error',\n",
    "#           cv = 10)  # cross validation\n",
    "\n",
    "# sfs = sfs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_sfs(sfs.get_metric_dict(), kind = 'std_err')\n",
    "# # Wide blue band --  different combinations of each number of features\n",
    "# # Dark blue dots -- average performance\n",
    "# fig.set_size_inches(18.5, 4, forward=True)\n",
    "# plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "# plt.xticks(fontsize = 'small', rotation = 90)\n",
    "# plt.axvline(x = 36)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it seems that when the number of features is about 36 the performance stops increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by Forward Stepwise Feature Selection:\n",
      "(0, 2, 6, 21, 28, 35, 45, 47, 48, 59, 62, 63, 68, 71, 90, 125, 129, 133, 180, 271, 319, 333, 407, 408, 417, 434, 446, 448, 470, 476, 477, 480, 487, 511, 519, 573)\n"
     ]
    }
   ],
   "source": [
    "# # If I took only 36 features, what would be the features selected\n",
    "# num_feat = 36\n",
    "# scores = sfs.get_metric_dict()[num_feat]\n",
    "# # Selected features\n",
    "print('Features selected by Forward Stepwise Feature Selection:')\n",
    "# farward_selected_features = scores['feature_names']\n",
    "\n",
    "# Hard-code selected features so that not to re-compute them in development\n",
    "farward_selected_features = (0, 2, 6, 21, 28, 35, 45, 47, 48, 59, 62, 63, 68, 71, 90,\n",
    "                             125, 129, 133, 180, 271, 319, 333, 407, 408, 417, 434,\n",
    "                             446, 448, 470, 476, 477, 480, 487, 511, 519, 573)\n",
    "print(farward_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by Backward Variables Selection:\n",
      "[29, 33, 35, 44, 47, 61, 67, 96, 98, 117, 125, 126, 128, 129, 170, 181, 204, 302, 316, 333, 343, 407, 416, 443, 444, 446, 448, 459, 475, 477, 552, 559, 568, 571, 573, 585]\n"
     ]
    }
   ],
   "source": [
    "# # Recursive Feature Elimination for backward model selection\n",
    "# estimator = linear_model.LinearRegression()\n",
    "# selector = RFE(estimator, num_feat, step=1)#select 36 features. Step=1 means each step only remove 1 variable from the model\n",
    "# selector = selector.fit(X, Y)  # learn from dataset and do backward variable selection\n",
    "\n",
    "print('Features selected by Backward Variables Selection:')\n",
    "# backward_selected_features = X.columns[selector.support_]\n",
    "\n",
    "# Hard-code selected features so that not to re-compute them in development\n",
    "backward_selected_features = [ 29,  33,  35,  44,  47,  61,  67,  96,  98, 117, 125, 126, 128, 129,\n",
    "                              170, 181, 204, 302, 316, 333, 343, 407, 416, 443, 444, 446, 448, 459,\n",
    "                              475, 477, 552, 559, 568, 571, 573, 585]\n",
    "print(backward_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the LASSO regression method of feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names selected by LASSO:\n",
      "[0, 1, 2, 3, 21, 22, 23, 24, 55, 59, 88, 90, 162, 250, 271, 274, 294, 295, 296, 418, 419, 433, 439, 468, 482, 483, 484, 485, 486, 487, 488, 499, 500, 510, 511, 589]\n",
      "Number of features selected: 36\n"
     ]
    }
   ],
   "source": [
    "# # LASSO\n",
    "# alpha = 0.19 # Increasing alpha can shrink more variable coefficients to 0\n",
    "# clf = linear_model.Lasso(alpha=alpha)\n",
    "# clf.fit(X, Y)\n",
    "\n",
    "print('Feature names selected by LASSO:')\n",
    "# mask = (clf.coef_ != 0)\n",
    "# lasso_features = X.columns[mask]\n",
    "\n",
    "# Hard-code selected features so that not to re-compute them in development\n",
    "lasso_features = [  0,   1,   2,   3,  21,  22,  23,  24,  55,  59,  88,  90, 162, 250,\n",
    "                  271, 274, 294, 295, 296, 418, 419, 433, 439, 468, 482, 483, 484, 485,\n",
    "                  486, 487, 488, 499, 500, 510, 511, 589]\n",
    "print(lasso_features)\n",
    "print('Number of features selected:', len(lasso_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a set of the resulting three lists of selected features to get rid of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique features across the three lists: 90\n"
     ]
    }
   ],
   "source": [
    "final_features = set()\n",
    "for features_list in (backward_selected_features, farward_selected_features, lasso_features):\n",
    "    for feature_name in features_list:\n",
    "        final_features.add(feature_name)\n",
    "print('Number of unique features across the three lists:', len(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape after feature selection: (1253, 90)\n",
      "Testing set shape after feature selection: (314, 90)\n"
     ]
    }
   ],
   "source": [
    "final_features_list = list(final_features)\n",
    "X = X[final_features_list]\n",
    "XX = XX[final_features_list]\n",
    "print('Training set shape after feature selection:', X.shape)\n",
    "print('Testing set shape after feature selection:', XX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-scale the training and the testing sets\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)  # Fit to training set X\n",
    "# X_scaled = scaler.transform(X) # --> array\n",
    "X_scaled = pd.DataFrame(scaler.transform(X), \n",
    "                        index = X.index,\n",
    "                        columns = X.columns)\n",
    "# XX_scaled = scaler.transform(XX)  # --> array\n",
    "XX_scaled = pd.DataFrame(scaler.transform(XX),\n",
    "                         index = XX.index,\n",
    "                         columns = XX.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle the class imbalance problem\n",
    "\n",
    "Oversample the training set only, so that to avoid using information from the testing set to create synthetic observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of target labels, 0s and 1s\n",
      "Training set shape Counter({0: 1162, 1: 91})\n",
      "Testing set shape Counter({0: 301, 1: 13})\n"
     ]
    }
   ],
   "source": [
    "print('Distribution of target labels, 0s and 1s')\n",
    "print('Training set shape {}'.format(Counter(Y)))\n",
    "print('Testing set shape {}'.format(Counter(YY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of target labels, 0s and 1s\n",
      "Resampled training set shape Counter({0: 1162, 1: 1162})\n"
     ]
    }
   ],
   "source": [
    "# Re-sample training data\n",
    "sm = SMOTE(random_state = 42)\n",
    "X_resample, Y_resample = sm.fit_sample(X_scaled, Y)\n",
    "print('Distribution of target labels, 0s and 1s')\n",
    "print('Resampled training set shape {}'.format(Counter(Y_resample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X scaled and over-sampled: <class 'numpy.ndarray'> (2324, 90)\n",
      "Y over-sampled: <class 'numpy.ndarray'> (2324,)\n",
      "XX scaled: <class 'pandas.core.frame.DataFrame'> (314, 90)\n",
      "YY: <class 'numpy.ndarray'> (314,)\n"
     ]
    }
   ],
   "source": [
    "# Training: X_resample, Y_resample\n",
    "# Testing: XX_scaled, YY\n",
    "print('X scaled and over-sampled:', type(X_resample), X_resample.shape)\n",
    "print('Y over-sampled:', type(Y_resample), Y_resample.shape)\n",
    "print('XX scaled:', type(XX_scaled), XX_scaled.shape)\n",
    "print('YY:', type(YY), YY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1 Summary <a id='summary1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>initial dataset</b> contained <b>590 features</b>. After <b>removal of features with more than half values missing</b>, the number of features <b>reduced to 562</b>. The remaining missing values were imputed by using medians. Then I <b>removed constant-valued columns</b> (with standard deviation = 0) and <b>almost constant-valued columns</b> (with variance < 0.01). That reduced the number of features <b>to 275</b>.\n",
    "\n",
    "For <b>highly correlated features</b> and <b>features with a high mutual information score</b>, first I selected pairs of columns which had a correlation coefficient above 0.8 and pairs of columns which had a mutual information score above 0.9. Then I compared each column in the pairs to the target variable Y and removed that column which had a smaller impact on Y (either a smaller correlation coefficient or a smaller mutual information score, respectively). That <b>reduced the number of features to 173</b>.\n",
    "\n",
    "After that I applied <b>three feature selection methods</b> — Forward Stepwise Feature Selection, Backward Features Selection, and LASSO regression. By using these methods, I obtained three lists each containing 36 features. By taking a set of these three lists to remove duplicates I arrived at the <b>final dataset of 90 features</b> which are probably relevant for predicting the target label Y and which will be used in later models.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-name references to the training dataset for convenience\n",
    "X_train, y_train = X_resample, Y_resample\n",
    "# # Search for optimal model parameters for the decision tree\n",
    "# dt_parameters = {'criterion': ('gini', 'entropy'),\n",
    "#                  'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#                  'min_samples_leaf': [4, 5, 6, 7],\n",
    "#                  'min_samples_split': [2, 3, 4, 5],\n",
    "#                 }\n",
    "# dtc = DecisionTreeClassifier()\n",
    "# gridsearch_dt_clf = GridSearchCV(dtc, dt_parameters, cv=5, scoring='recall_macro')\n",
    "# gridsearch_dt_clf.fit(X_train, y_train)\n",
    "# gridsearch_dt_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT:\n",
    "\n",
    "default scoring\n",
    "\n",
    "{'criterion': 'entropy',<br/>\n",
    " 'max_depth': 10,<br/>\n",
    " 'min_samples_leaf': 7,<br/>\n",
    " 'min_samples_split': 2}\n",
    " \n",
    " scoring='recall_macro'\n",
    " \n",
    " {'criterion': 'entropy',<br/>\n",
    " 'max_depth': 10,<br/>\n",
    " 'min_samples_leaf': 6,<br/>\n",
    " 'min_samples_split': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search for optimal model parameters for the random forest\n",
    "# rf_parameters = {'n_estimators': [10, 30, 50, 70, 90, 110],\n",
    "#                  'max_depth': [4, 5, 6, 7, 8, 9, 10, None],\n",
    "#                  'min_samples_leaf': [1, 3, 4, 5, 6, 7],\n",
    "#                  'min_samples_split': [2, 3, 4, 5],\n",
    "#                  'max_features': [2, 3, 4],\n",
    "#                 }\n",
    "# rfc = RandomForestClassifier()\n",
    "# gridsearch_rf_clf = GridSearchCV(rfc, rf_parameters, cv=5)\n",
    "# gridsearch_rf_clf.fit(X_train, y_train)\n",
    "# gridsearch_rf_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT:\n",
    "    \n",
    "scoring='recall_macro'\n",
    "\n",
    "{'max_depth': None,<br/>\n",
    " 'max_features': 3,<br/>\n",
    " 'min_samples_leaf': 1,<br/>\n",
    " 'min_samples_split': 3,<br/>\n",
    " 'n_estimators': 90}\n",
    "\n",
    "default scoring\n",
    "\n",
    "{'max_depth': None,<br/>\n",
    " 'max_features': 2,<br/>\n",
    " 'min_samples_leaf': 1,<br/>\n",
    " 'min_samples_split': 2,<br/>\n",
    " 'n_estimators': 110}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search for optimal model parameters for the random forest\n",
    "# rf_parameters = {'n_estimators': [110, 120, 200],\n",
    "#                  'max_depth': [4, 5, 6, 7, 8, 9, 10, None],\n",
    "#                  'min_samples_leaf': [1, 3,  6, 7],\n",
    "#                  'min_samples_split': [2,  5],\n",
    "#                  'max_features': [2, 3, 4],\n",
    "#                 }\n",
    "# rfc = RandomForestClassifier()\n",
    "# gridsearch_rf_clf = GridSearchCV(rfc, rf_parameters, cv=5, scoring='recall_macro')\n",
    "# gridsearch_rf_clf.fit(X_train, y_train)\n",
    "# gridsearch_rf_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT:\n",
    "\n",
    "scoring='recall_macro'\n",
    "\n",
    " {'max_depth': None,<br/>\n",
    " 'max_features': 2,<br/>\n",
    " 'min_samples_leaf': 1,<br/>\n",
    " 'min_samples_split': 2,<br/>\n",
    " 'n_estimators': 120}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Search for parameters\n",
    "\n",
    "# # LinearSVC\n",
    "# lin_svc_params = {'C': (0.01, 0.1, 1, 10, 100, 1000)} \n",
    "# lin_svc = svm.LinearSVC(class_weight = 'balanced', max_iter=3000)\n",
    "# gridseacrh_lin_svc = GridSearchCV(lin_svc, lin_svc_params, cv = 5)\n",
    "# gridseacrh_lin_svc.fit(X_train, y_train)\n",
    "# gridseacrh_lin_svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT: {'C': 10} but issued a failed-to-converge warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters for SVC with kernel='linear'\n",
    "# svc_params = {'C': (0.01, 0.1, 1, 10, 100, 1000),\n",
    "#              'gamma': (0.001, 0.01, 1, 10, 100, 1000, 'scale')\n",
    "#              }\n",
    "# svc = svm.SVC(kernel='linear')\n",
    "# gridsearch_svc = GridSearchCV(svc, svc_params, cv = 5)\n",
    "# gridsearch_svc.fit(X_train, y_train)\n",
    "# gridsearch_svc.best_params_\n",
    "\n",
    "# THIS CODE SEEMS TO NEVER STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters for poly and rbf SVC\n",
    "# svc_params = {'C': (0.01, 0.1, 1, 10, 100, 1000),\n",
    "#               'kernel': ('poly', 'rbf'),\n",
    "#              'degree': (2, 3, 4),\n",
    "#              'gamma': (0.001, 0.01, 1, 10, 100, 1000, 'scale')\n",
    "#              }\n",
    "# svc = svm.SVC()\n",
    "# gridsearch_svc = GridSearchCV(svc, svc_params, cv = 5)\n",
    "# gridsearch_svc.fit(X_train, y_train)\n",
    "# gridsearch_svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT: \n",
    "\n",
    "default scoring and scoring='recall_macro'\n",
    "\n",
    "{'C': 1, 'degree': 4, 'gamma': 'scale', 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try a higher degree for poly SVC\n",
    "# svc_params = {'C': (1, 1000),\n",
    "#              'degree': (4, 5),\n",
    "#              'gamma': (0.001, 1000, 'scale')\n",
    "#              }\n",
    "# svc = svm.SVC(kernel='poly')\n",
    "# gridsearch_svc = GridSearchCV(svc, svc_params, cv = 5)\n",
    "# gridsearch_svc.fit(X_train, y_train)\n",
    "# gridsearch_svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT: {'C': 1, 'degree': 4, 'gamma': 'scale'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models by using the parameters found by grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "np.random.seed(101) # Ensure the decision tree is deterministic\n",
    "dec_tree = DecisionTreeClassifier(criterion = 'entropy',\n",
    "                                       max_depth = 10,  # Limit the # of decision levels\n",
    "                                       min_samples_leaf = 6,  # Limit the # of examples for a leaf\n",
    "                                       min_samples_split = 4,  # Limit the # of examples for a split\n",
    "                                      ).fit(X_train, y_train)  \n",
    "\n",
    "# Random forest\n",
    "rf_clf = RandomForestClassifier(n_estimators = 110,  # Number of trees\n",
    "                                max_depth = None,\n",
    "                                min_samples_split = 2,  # Limit the # of examples for a split\n",
    "                                max_features = 2,  # Consider x features at each split\n",
    "                                random_state = 0,\n",
    "                                verbose = 0,\n",
    "                                min_samples_leaf = 1,\n",
    "                               ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector classifier\n",
    "svc_clf = svm.SVC(kernel = 'poly',\n",
    "                  C = 1,\n",
    "                  gamma = 'scale',\n",
    "                  degree = 4,\n",
    "                  probability = True  # To enable predict_proba() method\n",
    "                 ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and evaluate on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-name references to the testing set for convenience\n",
    "X_test, y_test = XX_scaled, YY\n",
    "# Make predictions\n",
    "y_dc_pred = dec_tree.predict(X_test)\n",
    "y_rf_pred = rf_clf.predict(X_test)\n",
    "y_svc_pred = svc_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree model's accuracy on testing data is: 83.12%\n"
     ]
    }
   ],
   "source": [
    "# Generate the accuracy score for the decision tree model on testing data\n",
    "acc_dec_tree = accuracy_score(y_test, y_dc_pred) * 100\n",
    "print(\"Decision tree model's accuracy on testing data is: {:.2f}%\".format(acc_dec_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest model's accuracy on testing data is: 95.54%\n"
     ]
    }
   ],
   "source": [
    "# Generate the accuracy score for the random forest model on testing data\n",
    "acc_random_forest = accuracy_score(y_test, y_rf_pred) * 100\n",
    "print(\"Random forest model's accuracy on testing data is: {:.2f}%\".format(acc_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC's accuracy on testing data is: 95.54%\n"
     ]
    }
   ],
   "source": [
    "# Generate the accuracy score for the SVC model on testing data\n",
    "acc_svc = accuracy_score(y_test, y_svc_pred) * 100\n",
    "print(\"SVC's accuracy on testing data is: {:.2f}%\".format(acc_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... but Accuracy is not an appropriate measure to assess the model's performance in case of imbalanced datasets  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print confusion matrices -- poor results on the rare positive class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree model confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>256</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0          256           45\n",
       "True 1            8            5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Decision tree model confusion matrix')\n",
    "dec_tree_conf_matrix_obj = confusion_matrix(y_test, y_dc_pred)\n",
    "conf_matrix_dec_tree = pd.DataFrame(dec_tree_conf_matrix_obj,\n",
    "                                    columns = ['Predicted 0', 'Predicted 1'],\n",
    "                                    index = ['True 0', 'True 1']\n",
    "                                   )\n",
    "conf_matrix_dec_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest model confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0          300            1\n",
       "True 1           13            0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Random forest model confusion matrix')\n",
    "rf_conf_matrix_obj = confusion_matrix(y_test, y_rf_pred)\n",
    "conf_matrix_rf = pd.DataFrame(rf_conf_matrix_obj,\n",
    "                              columns = ['Predicted 0', 'Predicted 1'],\n",
    "                              index = ['True 0', 'True 1']\n",
    "                             )\n",
    "conf_matrix_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0          300            1\n",
       "True 1           13            0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('SVC confusion matrix')\n",
    "svc_conf_matrix_obj = confusion_matrix(y_test, y_svc_pred)\n",
    "conf_matrix_svc = pd.DataFrame(svc_conf_matrix_obj,\n",
    "                              columns = ['Predicted 0', 'Predicted 1'],\n",
    "                              index = ['True 0', 'True 1']\n",
    "                             )\n",
    "conf_matrix_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91       301\n",
      "           1       0.10      0.38      0.16        13\n",
      "\n",
      "    accuracy                           0.83       314\n",
      "   macro avg       0.53      0.62      0.53       314\n",
      "weighted avg       0.93      0.83      0.88       314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Decision tree report')\n",
    "print(classification_report(y_test, y_dc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       301\n",
      "           1       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.96       314\n",
      "   macro avg       0.48      0.50      0.49       314\n",
      "weighted avg       0.92      0.96      0.94       314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Random forest report')\n",
    "print(classification_report(y_test, y_rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       301\n",
      "           1       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.96       314\n",
      "   macro avg       0.48      0.50      0.49       314\n",
      "weighted avg       0.92      0.96      0.94       314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SVC report')\n",
    "print(classification_report(y_test, y_svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ROC curve to compare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decision tree\n",
    "y_pred_dt = dec_tree.predict_proba(X_test.values)[:, 1]\n",
    "fpr_dt, tpr_dt, _ = metrics.roc_curve(y_test.astype(int), y_pred_dt)\n",
    "auc_dt = metrics.auc(fpr_dt, tpr_dt)\n",
    "\n",
    "# Random forest\n",
    "y_pred_rf = rf_clf.predict_proba(X_test.values)[:, 1]  # proba for positive class\n",
    "fpr_rf, tpr_rf, _ = metrics.roc_curve(y_test.astype(int), y_pred_rf)\n",
    "auc_rf = metrics.auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# Support vector classifier\n",
    "y_pred_svc = svc_clf.predict_proba(X_test.values)[:, 1]\n",
    "fpr_svc, tpr_svc, _ = metrics.roc_curve(y_test.astype(int), y_pred_svc)\n",
    "auc_svc = metrics.auc(fpr_svc, tpr_svc)\n",
    "\n",
    "plt.figure()\n",
    "# reference line for random classifier\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label = 'Random classifier')\n",
    "plt.plot(fpr_dt, tpr_dt, label='Decision tree, AUC={:.2f}'.format(auc_dt))\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random forest, AUC={:.2f}'.format(auc_rf))\n",
    "plt.plot(fpr_svc, tpr_svc, label='SVC, AUC={:.2f}'.format(auc_svc))\n",
    "plt.xlabel('FALSE Positive Rate')\n",
    "plt.ylabel('TRUE Positive Rate')\n",
    "plt.title('ROC curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2 Summary <a id='summary2'></a>\n",
    "\n",
    "I built three models - a Decision Tree, a Random Forest, and an SVC, to predict faulty products, that is the rare positive class in the dataset. I have commented out some code in the notebook and hard-coded the results of calculations because otherwise the execution time is too long.\n",
    "\n",
    "Parameters for the models were found by grid search.\n",
    "\n",
    "The ability of the models to correctly predict the rare class on the testing data is very low. The Decision Tree has recall of 0.38 (by correctly predicting 5 positive cases out of 13), while both the Random Forest and the SVC fail to predict correctly any positive cases. \n",
    "\n",
    "As can be seen on the ROC curve chart above, the areas under curve of all the three models are not very different from each other and the curves are only slightly higher than the reference line for a random classifier.\n",
    "\n",
    "I suppose there can be many reasons for this poor performance, from methodological errors to the inappropriateness of the data itself.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3\n",
    "\n",
    "## Simple Neural Networks Model - Single-layer Perceptron <a id='slp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "https://www.tensorflow.org/guide/keras\n",
    "\n",
    "https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "Multilayer Perceptron (MLP) for binary classification\n",
    "\n",
    "https://www.tensorflow.org/tutorials/keras/overfit_and_underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reminder regarding the shape of my data:\n",
    "```\n",
    "X_train (2324, 90) <class 'numpy.ndarray'>\n",
    "y_train (2324,) <class 'numpy.ndarray'> Counter({0: 1162, 1: 1162})\n",
    "X_test (314, 90) <class 'pandas.core.frame.DataFrame'>\n",
    "y_test (314,) <class 'numpy.ndarray'> Counter({0: 301, 1: 13})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limited grid search can be done as follows:\n",
    "```python\n",
    "def create_model():\n",
    "    \"\"\"Return a model for grid search.\"\"\"\n",
    "    slp_model = keras.models.Sequential([\n",
    "        keras.layers.Dense(64, input_dim=90, activation='relu'),  # one input layer\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)           # one output layer\n",
    "    ])\n",
    "    \n",
    "    slp_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'binary_crossentropy'])\n",
    "    \n",
    "    return slp_model\n",
    "\n",
    "model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model)\n",
    "param_grid = dict(epochs=[20,30,40], batch_size=[10, 20, 40, 60, 80, 100])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X_train, y_train, verbose=0)\n",
    "grid_result.best_params_\n",
    "\n",
    "{'batch_size': 10, 'epochs': 40}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_slp():\n",
    "    \"\"\"Return predictions from a single run of perceptron.\"\"\"\n",
    "    slp_model = keras.models.Sequential([\n",
    "        keras.layers.Dense(64, input_dim=90, activation='relu'),  # one input layer\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)           # one output layer\n",
    "    ])\n",
    "\n",
    "    slp_model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "    slp_model.fit(X_train, \n",
    "                  y_train,\n",
    "                  epochs=40,\n",
    "                  batch_size=10,\n",
    "                  validation_split=0.1,\n",
    "                  verbose=0\n",
    "                 )\n",
    "\n",
    "    slp_predictions = slp_model.predict(X_test)\n",
    "    \n",
    "    return slp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "              0          mean\n",
      "0  5.215406e-06  5.215406e-06\n",
      "1  5.960464e-08  5.960464e-08\n",
      "2  2.533197e-06  2.533197e-06\n",
      "3  2.129972e-04  2.129972e-04\n",
      "4  3.784895e-06  3.784895e-06\n",
      "SLP classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       301\n",
      "           1       0.05      0.08      0.06        13\n",
      "\n",
      "    accuracy                           0.90       314\n",
      "   macro avg       0.51      0.51      0.51       314\n",
      "weighted avg       0.92      0.90      0.91       314\n",
      "\n",
      "SLP confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>283</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0          283           18\n",
       "True 1           12            1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate several arrays of predictions -- see results for 10 runs below\n",
    "slp_predictions = []\n",
    "num_runs = 1  # make it 1 for submission so that the code runs fast\n",
    "for i in range(num_runs):\n",
    "    slp_predictions.append(run_slp())\n",
    "\n",
    "# Calculate the mean of the predictions\n",
    "all_predictions = np.concatenate(slp_predictions, axis=1)\n",
    "slp_pred_df = pd.DataFrame(data=all_predictions,\n",
    "                           index=np.arange(all_predictions.shape[0]),\n",
    "                           columns=np.arange(all_predictions.shape[1])\n",
    "                          )\n",
    "slp_pred_df['mean'] = np.mean(slp_pred_df.loc[:, :], axis=1)\n",
    "print(slp_pred_df.head())\n",
    "\n",
    "# Get classifications\n",
    "slp_classifications = np.where(slp_pred_df['mean'] >= 0.5, 1, 0)\n",
    "\n",
    "print('SLP classification report')\n",
    "print(classification_report(y_test, slp_classifications))\n",
    "\n",
    "print('SLP confusion matrix')\n",
    "slp_conf_matrix_obj = confusion_matrix(y_test, slp_classifications)\n",
    "conf_matrix_slp = pd.DataFrame(slp_conf_matrix_obj,\n",
    "                              columns = ['Predicted 0', 'Predicted 1'],\n",
    "                              index = ['True 0', 'True 1']\n",
    "                             )\n",
    "conf_matrix_slp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Average results for 10 runs for an SLP model</b>:\n",
    "\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "               0       0.96      0.95      0.96       301\n",
    "               1       0.12      0.15      0.13        13\n",
    "\n",
    "        accuracy                           0.92       314\n",
    "       macro avg       0.54      0.55      0.54       314\n",
    "    weighted avg       0.93      0.92      0.92       314\n",
    "\n",
    "SLP confusion matrix\n",
    "\n",
    "    Predicted 0\tPredicted 1\n",
    "\n",
    "    True 0\t286\t15\n",
    "\n",
    "    True 1\t11\t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Deep Neural Network (DNN) model - Multilayer Perceptron (MLP) <a id='mlp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlp():\n",
    "    \"\"\"Return predictions from a single run of MLP.\"\"\"\n",
    "    mlp_model = keras.models.Sequential([\n",
    "        keras.layers.Dense(64, input_dim=90, activation='relu'),  # Input layer\n",
    "#         keras.layers.Dropout(0.5),                                \n",
    "        keras.layers.Dense(64, activation='relu'),                # Hidden layer\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)           # Output layer\n",
    "    ])\n",
    "\n",
    "    mlp_model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "    mlp_model.fit(X_train, \n",
    "                  y_train,\n",
    "                  epochs=20,\n",
    "                  batch_size=32,\n",
    "                  validation_split=0.1,\n",
    "                  verbose=0\n",
    "                 )\n",
    "\n",
    "    mlp_predictions = mlp_model.predict(X_test)\n",
    "    \n",
    "    return mlp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0             1         2             3         4             5  \\\n",
      "0  1.478791e-04  1.648068e-05  0.000016  2.125204e-04  0.000022  4.280508e-04   \n",
      "1  0.000000e+00  5.960464e-08  0.000000  8.940697e-08  0.000000  0.000000e+00   \n",
      "2  2.564579e-03  2.583861e-05  0.008973  1.657605e-04  0.000181  6.854534e-07   \n",
      "3  5.662441e-07  1.400709e-06  0.003397  5.725026e-05  0.001931  5.453825e-06   \n",
      "4  1.192093e-07  0.000000e+00  0.000003  3.334880e-05  0.013774  2.834797e-04   \n",
      "\n",
      "              6             7             8             9          mean  \n",
      "0  3.039837e-06  2.682209e-07  3.206730e-05  6.526709e-05  9.430051e-05  \n",
      "1  5.960464e-08  0.000000e+00  1.192093e-07  1.788139e-07  5.066395e-08  \n",
      "2  6.341934e-05  6.854534e-07  3.081560e-05  4.231930e-06  1.200923e-03  \n",
      "3  5.203992e-03  1.117885e-04  3.856421e-05  9.199083e-04  1.166731e-03  \n",
      "4  7.465482e-05  6.347895e-06  4.226565e-04  1.475215e-05  1.461187e-03  \n",
      "MLP classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       301\n",
      "           1       0.11      0.15      0.13        13\n",
      "\n",
      "    accuracy                           0.91       314\n",
      "   macro avg       0.54      0.55      0.54       314\n",
      "weighted avg       0.93      0.91      0.92       314\n",
      "\n",
      "MLP confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>285</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0          285           16\n",
       "True 1           11            2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate several arrays of predictions -- see average results for 10 runs below\n",
    "mlp_predictions = []\n",
    "num_runs = 1  # make it 1 for submission so that the code runs fast\n",
    "for i in range(num_runs):\n",
    "    mlp_predictions.append(run_mlp())\n",
    "\n",
    "# Calculate the mean of the predictions\n",
    "all_predictions_mlp = np.concatenate(mlp_predictions, axis=1)\n",
    "mlp_pred_df = pd.DataFrame(data=all_predictions_mlp,\n",
    "                           index=np.arange(all_predictions_mlp.shape[0]),\n",
    "                           columns=np.arange(all_predictions_mlp.shape[1])\n",
    "                          )\n",
    "mlp_pred_df['mean'] = np.mean(mlp_pred_df.loc[:, :], axis=1)\n",
    "print(mlp_pred_df.head())\n",
    "\n",
    "# Get classifications\n",
    "mlp_classifications = np.where(mlp_pred_df['mean'] >= 0.5, 1, 0)\n",
    "\n",
    "print('MLP classification report')\n",
    "print(classification_report(y_test, mlp_classifications))\n",
    "\n",
    "print('MLP confusion matrix')\n",
    "mlp_conf_matrix_obj = confusion_matrix(y_test, mlp_classifications)\n",
    "conf_matrix_mlp = pd.DataFrame(mlp_conf_matrix_obj,\n",
    "                              columns = ['Predicted 0', 'Predicted 1'],\n",
    "                              index = ['True 0', 'True 1']\n",
    "                             )\n",
    "conf_matrix_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Average results for 10 runs of an MLP model</b>\n",
    "\n",
    "MLP classification report\n",
    "\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "               0       0.96      0.95      0.96       301\n",
    "               1       0.12      0.15      0.13        13\n",
    "    \n",
    "        accuracy                           0.92       314\n",
    "       macro avg       0.54      0.55      0.54       314\n",
    "    weighted avg       0.93      0.92      0.92       314\n",
    "\n",
    "MLP confusion matrix\n",
    "\n",
    "    Predicted 0\tPredicted 1\n",
    "\n",
    "    True 0\t286\t15\n",
    "\n",
    "    True 1\t11\t2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model <a id='rnn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References re. how to transform data for RNN modeling:\n",
    "\n",
    "https://www.kaggle.com/amirrezaeian/time-series-data-analysis-using-lstm-tutorial\n",
    "\n",
    "https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data containing the selected features into training & testing \n",
    "reduced_prod_df = production_df[final_features_list]\n",
    "train_X, test_X = reduced_prod_df.iloc[:1200, :], reduced_prod_df.iloc[1200:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPUTING OUTLIERS\n",
      "Done\n",
      "IMPUTING OUTLIERS\n",
      "Done\n",
      "Number of missing values in training set X: 1144\n",
      "IMPUTING MISSING VALUES\n",
      "Number of missing values in training set X: 0\n",
      "Number of missing values in testing set XX: 640\n",
      "IMPUTING MISSING VALUES\n",
      "Number of missing values in testing set XX: 0\n",
      "(1200, 90) (367, 90)\n"
     ]
    }
   ],
   "source": [
    "# Impute outliers and missing values\n",
    "train_X, test_X = prepare_data(['impute_outliers', 'impute_missing'],\n",
    "                               train_X,\n",
    "                               test_X)\n",
    "print(train_X.shape, test_X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-scale the training and the testing sets\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)  # Fit to training set X\n",
    "train_X = pd.DataFrame(scaler.transform(train_X),\n",
    "                       index = train_X.index,\n",
    "                       columns = train_X.columns)\n",
    "test_X = pd.DataFrame(scaler.transform(test_X),\n",
    "                      index = test_X.index,\n",
    "                      columns = test_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "# Source: see above\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform data to a format appropriate for RNN\n",
    "train_X = series_to_supervised(train_X.values, 1, 1)\n",
    "test_X = series_to_supervised(test_X.values, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 180) (1200,) (366, 180) (367,)\n"
     ]
    }
   ],
   "source": [
    "# Split labels into training & testing sets\n",
    "train_y, test_y = production_df.iloc[:1200, -1], production_df.iloc[1200:, -1]\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of pass/fail labels in the training set\n",
      "Pass labels -1: 1118 cases\n",
      "Fail labels 1: 82 cases\n",
      "Distribution of pass/fail labels in the testing set\n",
      "Pass labels -1: 345 cases\n",
      "Fail labels 1: 22 cases\n"
     ]
    }
   ],
   "source": [
    "# Distribution of positive and negative cases in the train and test data\n",
    "train_y = train_y.astype(int)\n",
    "mask_neg_outcome_Y = (train_y == -1)  # --> bool array\n",
    "pos_outcome  = (train_y == 1)\n",
    "print('Distribution of pass/fail labels in the training set')\n",
    "print('Pass labels -1:', sum(mask_neg_outcome_Y), 'cases')\n",
    "print('Fail labels 1:', sum(pos_outcome), 'cases')\n",
    "\n",
    "test_y = test_y.astype(int)\n",
    "mask_neg_outcome_y = (test_y == -1)  # --> bool array\n",
    "pos_outcome  = (test_y == 1)\n",
    "print('Distribution of pass/fail labels in the testing set')\n",
    "print('Pass labels -1:', sum(mask_neg_outcome_y), 'cases')\n",
    "print('Fail labels 1:', sum(pos_outcome), 'cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass labels 0: 1118 cases\n",
      "Fail labels 1: 82 cases\n",
      "Pass labels 0: 345 cases\n",
      "Fail labels 1: 22 cases\n"
     ]
    }
   ],
   "source": [
    "# Replace -1 with 0\n",
    "train_y[mask_neg_outcome_Y] = 0\n",
    "zero_outcome = (train_y == 0)\n",
    "pos_outcome  = (train_y == 1)\n",
    "print('Pass labels 0:', sum(zero_outcome), 'cases')\n",
    "print('Fail labels 1:', sum(pos_outcome), 'cases')\n",
    "\n",
    "test_y[mask_neg_outcome_y] = 0\n",
    "zero_outcome = (test_y == 0)\n",
    "pos_outcome  = (test_y == 1)\n",
    "print('Pass labels 0:', sum(zero_outcome), 'cases')\n",
    "print('Fail labels 1:', sum(pos_outcome), 'cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 180) (1200,) (366, 180) (367,)\n"
     ]
    }
   ],
   "source": [
    "# To arrays\n",
    "train_X, train_y = train_X.values, train_y.values\n",
    "test_X, test_y = test_X.values, test_y.values\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 1, 180) (1200,) (366, 1, 180) (367,)\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data structure types\n",
    "[type(i) for i in (train_X, train_y,  test_X, test_y)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first rows in y-sets corresponding to the removed rows with NaN values \n",
    "# in the x-sets because of the pd.shift operation\n",
    "# train_X = np.delete(train_X, (0), axis=0)  # already deleted\n",
    "train_y = np.delete(train_y, (0), axis=0)\n",
    "# test_X = np.delete(test_X, (0), axis=0)  # already deleted\n",
    "test_y = np.delete(test_y, (0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn():\n",
    "    \"\"\"Return predictions from a single run of RNN.\"\"\"\n",
    "    rnn_model = keras.models.Sequential()\n",
    "    rnn_model.add(keras.layers.LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    rnn_model.add(keras.layers.Dropout(0.2))\n",
    "    #    model.add(keras.layers.LSTM(70))\n",
    "    #    model.add(keras.layers.Dropout(0.3))\n",
    "    rnn_model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "    # rnn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    rnn_model.compile(optimizer='adam',\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy', 'binary_crossentropy'])\n",
    "    rnn_model.fit(train_X,\n",
    "                  train_y,\n",
    "                  epochs=4,\n",
    "                  batch_size=70, \n",
    "#                     validation_data=(test_X, test_y),\n",
    "                  validation_split=0.1,\n",
    "                  verbose=2,\n",
    "                  shuffle=False)\n",
    "    rnn_predictions = rnn_model.predict(test_X)\n",
    "    return rnn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 1079 samples, validate on 120 samples\n",
      "Epoch 1/4\n",
      " - 1s - loss: 0.7079 - acc: 0.4940 - binary_crossentropy: 0.7079 - val_loss: 0.8576 - val_acc: 0.0667 - val_binary_crossentropy: 0.8576\n",
      "Epoch 2/4\n",
      " - 0s - loss: 0.5637 - acc: 0.8109 - binary_crossentropy: 0.5637 - val_loss: 0.7513 - val_acc: 0.2583 - val_binary_crossentropy: 0.7513\n",
      "Epoch 3/4\n",
      " - 0s - loss: 0.4788 - acc: 0.9231 - binary_crossentropy: 0.4788 - val_loss: 0.6541 - val_acc: 0.6833 - val_binary_crossentropy: 0.6541\n",
      "Epoch 4/4\n",
      " - 0s - loss: 0.4017 - acc: 0.9286 - binary_crossentropy: 0.4017 - val_loss: 0.5659 - val_acc: 0.9000 - val_binary_crossentropy: 0.5659\n",
      "          0      mean\n",
      "0  0.460808  0.460808\n",
      "1  0.486073  0.486073\n",
      "2  0.397704  0.397704\n",
      "3  0.431037  0.431037\n",
      "4  0.359273  0.359273\n",
      "RNN classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       344\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.93       366\n",
      "   macro avg       0.47      0.50      0.48       366\n",
      "weighted avg       0.88      0.93      0.91       366\n",
      "\n",
      "RNN confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0          342            2\n",
       "True 1           22            0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate several arrays of predictions -- see average results for 10 runs below\n",
    "rnn_predictions = []\n",
    "num_runs = 1  # make it 1 for submission so that the code runs fast\n",
    "for i in range(num_runs):\n",
    "    rnn_predictions.append(run_rnn())\n",
    "\n",
    "# Calculate the mean of the predictions\n",
    "all_predictions_rnn = np.concatenate(rnn_predictions, axis=1)\n",
    "rnn_pred_df = pd.DataFrame(data=all_predictions_rnn,\n",
    "                           index=np.arange(all_predictions_rnn.shape[0]),\n",
    "                           columns=np.arange(all_predictions_rnn.shape[1])\n",
    "                          )\n",
    "rnn_pred_df['mean'] = np.mean(rnn_pred_df.loc[:, :], axis=1)\n",
    "print(rnn_pred_df.head())\n",
    "\n",
    "# Get classifications\n",
    "rnn_classifications = np.where(rnn_pred_df['mean'] >= 0.5, 1, 0)\n",
    "print('RNN classification report')\n",
    "print(classification_report(test_y, rnn_classifications))\n",
    "\n",
    "print('RNN confusion matrix')\n",
    "rnn_conf_matrix_obj = confusion_matrix(test_y, rnn_classifications)\n",
    "conf_matrix_rnn = pd.DataFrame(rnn_conf_matrix_obj,\n",
    "                               columns = ['Predicted 0', 'Predicted 1'],\n",
    "                               index = ['True 0', 'True 1']\n",
    "                              )\n",
    "conf_matrix_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Average results for 10 runs of the RNN model</b>\n",
    "```\n",
    "RNN classification report\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      1.00      0.97       344\n",
    "           1       0.00      0.00      0.00        22\n",
    "\n",
    "    accuracy                           0.94       366\n",
    "   macro avg       0.47      0.50      0.48       366\n",
    "weighted avg       0.88      0.94      0.91       366\n",
    "\n",
    "RNN confusion matrix\n",
    "\n",
    "Predicted 0\tPredicted 1\n",
    "True 0\t344\t0\n",
    "True 1\t22\t0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3 summary <a id='summary3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I built three models - a simple perceptron model, a multi-layer perceptron and a recurrent neural network model (RNN). The RNN model required futher data processing in addition to that done in Milestone 1. \n",
    "\n",
    "All models perform very badly on the testing data by almost completely failing to predict correctly the rare positive class targets.\n",
    "\n",
    "Compared to the other models, only the Decision Tree model looks promising. \n",
    "\n",
    "In general,  based on the results of these models, I think it is not possible to draw any relevant conclusions about the relationship of the features and the product quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
